\section{Preliminaries : Motivations}\label{sec:bayprelim}


In the following, we will be interested by the multidimensional set of parameters $\theta$ of a GEV model, i.e. $\theta'=(\mu,\sigma,\xi)$ in a stationary context, where $\nu=\log \sigma$ is often used instead of $\sigma$ to get rid of its positivity constraint.
 Unless stated differently, we will write $\theta$ as this $3$D vector to facilitate readability.
 
We already argued that likelihood-based methods have desirable properties for inference, and hence we have adopted in \hyperref[sec::1]{Chapter \textbf{\ref{sec::1}}} and \textbf{\ref{sec::3}} the methods of (generalized or penalized) maximum likelihood to compute our estimators. Bayesian inference introduces a flexible alternative that also relies on the likelihood. We present the most fundamental result for Bayesian inference which is a direct interpretation of the Bayes' Theorem.

\begin{definition}[Posterior distribution]
	Let $\boldsymbol{x}=(x_1,\dots,x_n)$
	denote the vector of $n$ independent observations of a random variable $X$ with density $f(x|\theta)$, and let $\pi(\theta)$ denote the density of the prior distribution for $\theta$. According to Bayes' theorem, the \textbf{posterior distribution} of $\theta$ is 
	\begin{equation}\label{bayeseq}
	\pi (\theta|\boldsymbol{x})=\frac{\pi(\theta)\cdot L(\theta|\boldsymbol{x})}{\int_{\Theta} 
		\pi(\theta)\cdot L(\theta|\boldsymbol{x}) \cdot d\theta}\propto \pi(\theta)\cdot 
	L(\theta|\boldsymbol{x}),
	\end{equation}
	where $L(\theta|\boldsymbol{x})=\prod_i f(x_i|\theta)$ is the likelihood since the $x_i$'s are independent.
\end{definition}
This result (\ref{bayeseq}) provides the probabilistic framework to convert an initial set of beliefs about $\theta$, represented by the prior $\pi(\theta)$, into a posterior distribution that will be convenient to use for inference.
Relying on the posterior distribution to compute an estimate $\hat{\theta}$ of $\theta$ provides interesting characteristics :
\begin{itemize}
	\item Whenever possible, it allows to introduce an other source of knowledge coming from the domain, by the elicitation of a prior (see \hyperref[sec:prior]{Section \textbf{\ref{sec:prior}}}). However, it also introduces subjectiveness and antagonists contend that, since different analysts would specify
	different priors, all conclusions become meaninglessly subjective.
	
	\item\label{it2bayes} Accounting for  uncertainty is handled most easily in the
	Bayesian paradigm relying on distribution's properties of $\pi(\theta|\boldsymbol{x})$, for example its variance. %\cite[pp.106]{dey_extreme_2016}
	 It permits an elegant way of making future predictions, which is the most important issue in EVT.
	
	\item Bayesian framework can overcome the regularity conditions of the likelihood inference
	(see \hyperref[likgevintro]{Section \textbf{\ref{likgevintro}}}) by providing a viable alternative in cases when ML breaks down. Actually, we will see that the annual maximum temperatures we will consider are not far from the problematic cases ($\xi<-0.5$). 
	Moreover, Bayesian confidence intervals (i.e. \emph{credible} intervals, see \hyperref[bayes_cred_int]{Section \textbf{\ref{bayes_cred_int}}}) require no more need to fall to asymptotic theory.
\end{itemize}
In EVT, Bayesian inference is a wide expanding domain that has nearly infinite possibilities 
With strong probabilistic foundations, this method now mostly relies on computational capabilities, this 


\section{Prior Elicitation}\label{sec:prior}


Sometimes viewed as its greatest strength of Bayesian inference from the amount of information that can be retrieved, and sometimes viewed as its main pitfall due to the unquantifiable subjectivity that is introduced, the construction of the prior is at the center of Bayesian analysis.
Priors are necessary in the Bayesian paradigm to compute the posterior in  (\ref{bayeseq}). It requires the legitimate statement of a domain's expert, to make this viewed the less subjective as possible.

Priors may not be of great importance if the number $n$ of observations is large. From (\ref{bayeseq}) we see that the amount of information contained in the data through $L(\theta|\boldsymbol{x})$ will be prominent compared to this contained in the prior $\pi(\theta)$.
However, this configuration is rare in EVT where we are dealing with small constructed datasets. For this reason, incorporating external information through the prior should be (rigorously) studied. This type of prior is thus called \emph{informative priors}, while those who give most weights to data are called \emph{non-informative} (or \emph{objective}) \emph{priors}. We will focus on priors that are widely applied in EVT.




\subsection{Trivariate Normal Distribution}\label{sec:trivnorm}

The trivariate normal prior distribution on $\theta'=(\mu, \nu,\xi)$ leads to the following prior density 

\begin{equation}
\pi(\theta) \propto \sigma^{-1}\cdot \exp\bigg\{-\frac{1}{2}(\theta'-\boldsymbol{m})'\Sigma^{-1}(\theta'-\boldsymbol{m})\bigg\},
\end{equation}
where the mean vector $\boldsymbol{m}$ and the symmetric positive definite $[3\times 3]$ covariance matrix $\Sigma$ must be specified. This approach was used by \citet{coles_1996_bay} but other parametrizations exist. For example, changing $\mu$ to $\log \mu$ can be useful if a physical lower bound for this parameter must be specified.


\subsection{Gamma Distributions for Quantile Differences}

This method constructs priors on the quantile space, for fixed probabilities. Let $\text{Pr}(X>q_p)=p$, where $X$ has a GEV distribution (\ref{gevgen}). Then,

\begin{equation}
q_p=\mu + \sigma\cdot\xi^{-1}\cdot\big(x_p^{-\xi}-1\big),
\end{equation}
where $x_p=-\log (1-p)$. Indeed, we notice that it is a $m$-return level (\ref{rleqgev}) with $p=m^{-1}$. The prior distribution is constructed in terms of the quantiles $(q_{p_1},q_{p_2}, q_{p_3})$ for specified probabilities $p_1>p_2>p_3$. It is easier to work with the differences $(\tilde{q}_{p_1},\tilde{q}_{p_2}, \tilde{q}_{p_3})$, with $\tilde{q}_{p_i}-\tilde{q}_{p_{i-1}}$, $i=1,2,3$, where $q_{p0}$ is the physical lower endpoint of the process variable. We can take priors on the quantile differences to be independent with 

\begin{equation}
q
\end{equation}


\subsection{Beta Distributions for Probability Ratios}






Moreover, it can be a huge help with computation to have less diffuse priors, even
if they're not informative enough to have a noticeable impact on the posterior.


\subsection{Non-informative Priors}

Receive a correct and accepted advice from an expert is often difficult.
So, in many cases, it is not possible to inject information through the prior. 
Hence, priors must be constructed to represent this lack of knowledge and not influence posterior inferences. 

Parameters of the prior distribution are often called \emph{tuning parameters} or \emph{hyperparameters} to emphasize that we can tune the amount of information provided through the prior. In practical applications with  Markov Chains (see \hyperref[sec:baymcmc]{Section \textbf{\ref{sec:baymcmc}}}), these values are often tuned to who maximize the convergence of the posterior to its stationary target distribution. However, adjustments of these priors must always be thought in practical applications since objectives can be multiple. For example, if we take $\pi(\theta)\sim\mathcal{N}(\mu,\sigma^2)$,  the hyperparameter $\sigma^2$ will be tuned to represent the amount of knowledge we want to incorporate. It will tend to infinity to be non-informative (vague prior).


There exists a vast amount of uninformative priors in the literature (see e.g. \cite{yang_catalog_1996}, \cite{ni_noninformative_2003}, etc).
This family of priors can be \emph{improper}, i.e. priors for which the integral of $\pi(\theta)$ over the parameter space is not finite\footnote{It is valid to use improper priors only if the posterior target distribution is proper.}. 


\subsubsection*{Jeffrey's prior} 
Discovered by \citet{Jeffreys61a}, this prior is specified as 

\begin{equation}
\pi(\theta)\propto \sqrt{\det I(\theta)}, \qquad \text{where }\quad I_{ij}(\theta)=\mathbb{E}_{\theta}\Bigg[-\frac{\partial^2\log f(X|\theta)}{\partial\theta_i\partial\theta_j}\Bigg]; \ \ i,j= 1,\dots,d.
\end{equation}
%where $f(\boldsymbol{x}|\theta)$ is the density function of X.
This prior is a standard starting rule for an objective analysis.
This prior is invariant to reparametrization, but has a complex form for GEV models, and it exists only when $\xi>-0.5$ in GEV models, where it is function of $\xi$ and $\sigma$ only.


\subsubsection*{Maximal Data Information  prior}
Maximal Data Information (MDI) priors are defined to provide maximal average data information on $\theta$. These are not invariant under reparametrization but are easy to implement. It is generally defined as 

\begin{equation*}
\pi(\theta)\propto \exp\Big\{ \mathbb{E}\big[\log f(X|\theta
)\big]\Big\}.
\end{equation*}
However, it has been showed by \cite{northrop_posterior_2016} that 
both Jeffrey and MDI priors give improper posterior when there are no truncation of the shape parameter, and hence we must restrict the fact that $\pi(\theta)\rightarrow\infty$ as $\xi\rightarrow(-)\infty$ for Jeffreys (MDI), in order to obtain a proper posterior.

\subsubsection*{Vague priors}
The preferred alternative is often to construct uninformative priors by using proper priors which are near flat, e.g. which are uniform or which exhibits very large variance.
In GEV, we will often take independent normally distributed priors each with a large (tuned) variance. When these variances increase, we get at the limit

\begin{equation}
\pi(\theta)=\pi(\mu,\nu,\xi)\stackrel{(\independent)}{=}\pi(\mu)\cdot\pi(\nu)\cdot\pi(\xi)\ \propto 1,
\end{equation}
where $\nu= \log\sigma$.

Taking multivariate normal distribution as prior has also been proposed (see \hyperref[sec:trivnorm]{Section \textbf{\ref{sec:trivnorm}}} ) is often difficult as it involves 9 hyperparameters in total ($7$ in $\Sigma$, $\mu$ and $\xi$) and  and this can be difficult to address





\section{Bayesian Computation : Markov Chains}\label{sec:baymcmc}


Methods have been developed for sampling from arbitrary posterior distributions $\pi(\theta|\boldsymbol{x})$. Simulations of $N$ values $\theta_1,\theta_2,\dots,\theta_N$ that are iid from $\pi(\theta|\boldsymbol{x})$ can be used to estimate features of interest.

But simulating from $\pi(\theta|\boldsymbol{x})$ is usually not achievable and this is why we need \textbf{Markov Chain Monte Carlo} (MCMC) techniques. 
We use it to simulate a markov chain $\theta_1,\theta_2,\dots,\theta_N$ that conerge to the target distribution $\pi(\theta|\boldsymbol{x})$.
This means that, after some \emph{burn-in period} $B$, $\theta_{B+1},\dots,\theta_N$ can be treated as random sample from $\pi(\theta|\boldsymbol{x})$.


Let's now (a bit weakly) define one of the most important results in Markov Chain theory.

\begin{definition}[\emph{First-order discrete-time} Markov Property]
	Let $k_0,k_1,\dots$ be the states associated to a sequence of time-homogeneous random variables, say $\big\{\theta_t:t\in\mathbb{N}\big\}$.
	The Markov property states that the distribution of the future state $\theta_{t+1}$ depends only on the distribution of the current state $\theta_{t}$. 
	In other words, given $\theta_{t}$, we have that $\theta_{t+1}$ is independent of all the states prior to $t$. We can write this as
	
	\begin{equation}
	\text{\emph{Pr}}\big\{\theta_{t+1}=k_{t+1}\ |\ \theta_t=k_t,\ \theta_{t-1}=k_{t-1},\dots\big\} = \text{\emph{Pr}}\big\{\theta_{t+1}=k_{t+1}\ | \ \theta_t=k_t\big\}.
	\end{equation}
\end{definition} 

or see \citet[section 2.2.3]{angelino_patterns_2016} for more in-depth results.

The samples are not independent, and the dependence influences the accuracy of the posterior estimates. As dependence becomes stronger, we must increase the run-length $N$ to achieve the same accuracy. 


\subsection{Algorithms} 

We are looking for a so-generated chain that has a \underline{stationary} distribution $\pi(\theta|\boldsymbol{x})$. This is the case if the chain is 

\begin{enumerate}
	\item \emph{aperiodic}
	\item \emph{irreducible} or \emph{ergodic}, that is if any state for $\theta$ can be reached with probability $>0$ in a finite number of steps from any other state for $\theta$.
\end{enumerate}

"The Markov chains Stan and other MCMC samplers generate are \emph{ergodic} in the
sense required by the Markov chain central limit theorem, meaning roughly that there
is a reasonable chance of reaching one value of theta from another." \cite{stan_stan_2016}

With MH or Gibbs sampler, we need to tune individually the proposal standard deviations to reach a correct acceptance, and this is often done with trial-and-error methodology. 



The  performance  of  the standard  Markov  chain  Monte  Carlo  estimators  depends  on  how  effectively the Markov transition guides the Markov chain along the neighborhoods of high probability.  If  the  exploration  is  slow  then  the  estimators  will  become  computationally inefficient,  and  if  the  exploration  is  incomplete  then  the  estimators  will  become  biased
\citet{betancourt_diagnosing_2016}. It is then necessary to consider other form of sampling...


\subsection{Hamiltonian Monte Carlo}

Package Rstan

\cite{neal_mcmc_2011} and \cite{betancourt_hamiltonian_2015} are really 


HMC permit to better exploit the properties of
the target distribution to make informed jumps through neighborhoods of high probability while avoiding neighborhoods of low probability entirely.


\subsection{Computational efficiency comparison}

In modern statistical area, computing methods have been widely ... 
And this need for compiutations will rise in the future. 

We will then compare our 3 methods too see if effectively


\section{Convergence Diagnostics}

When applying MCMC algorithms to estimate posterior distributions, it is vital to assess convergence of the algorithm to try to ensure that we reached the stationary target distribution. Let's now enumerate some of the key steps we must keep in mind when thinking about convergence, an hence reliable results.

\begin{enumerate}
	\item A sufficient \emph{burn-in period} $B<N$ must be chosen to ensure that the convergence to the posterior distribution $\pi(\theta|\boldsymbol{x})$ has occurred. 
	\item For the same reason, a sufficient number of simulations $N$ to eliminate the influence of initial conditions and ensure accuracy in the estimations ((and then make sure than we are sampling from the target stationary (posterior) distribution)).
	\item Several dispersed starting values must have been simulated to ensure we explored all the regions of high probability. This is particularly important when the target distribution is complex.
	\item\label{convdiag4} The chains must have good mixing properties, in the sense that the whole parameter space (...) 
	A common technique that we will apply is to run different chains several times and then combine a proportion of each chain (typically $50\%$) to get the final chain. This procedure wants to ensure a proper mixing behaviour. 
	The potential scale reduction factor (Gelman diagnostic) is also a popular tool, see .
\end{enumerate}
We must keep in mind that no convergence diagnostics can prove that convergence really happened and validate the "model".
However, a combined use of several relevant diagnostics will be required to increase our confidence that convergence actually happened.

\subsection{Proposal Distribution}



The main ideas are : 

\begin{itemize}
	\item If the variance of the proposal distribution is too large, most proposals will be rejected : 
	
	 ie the jumps through the chain are too large,
	\item If the variance of the proposal distribution is too low, then most proposals will be accepted
\end{itemize}
Both are harmful for the objective of an efficient "visit" of the whole parameter space. 


Widely speaking, we consider 2 different  types of algorithms in which it is preferable to target a certain acceptance rate. It is distinguished by the updating manner of the components of $\theta$ through the algorithm, i.e. the 3 univariate parameters of interest.


\begin{itemize}
	\item When all components of $\theta$ are updated simultaneously, it is recommended to target an acceptance rate of around 0.20.
	 \citet{Roberts_weak_1997} have shown that, under quite general conditions, the asymptotically optimal acceptance rate is 0.234. (for  target density that has a symmetric product form ) 
	This quantity has been verified by \citet{Sherlock_optimal_2009}. It holds for the \emph{Metropolis-Hastings} algorithm.
	
	\item When the components are updated one at a time, an acceptance rate of around 0.40 is recommended. It holds for the\emph{ Gibbs sampler} algorithm. 
\end{itemize}
Let's  (see \citet{BÃ©dard_optimal_2008} for example for the first case)



\subsubsection*{Gelman-Rubin diagnostic : the $\hat{R}$ statistic}

As discussed in \hyperref[convdiag4]{item \textbf{4}} above

\subsubsection*{Geweke diagnostic}



\subsubsection*{Thinning}

iteration $k$ is stored only if $k \mod$ thin is zero (and if $k$ greater than or equal to the burn-in B).

This typically reduces the precision of posterior estimates, but it may represent a necessary computational saving.


\subsection{The problem of auto and cross-correlations in the chains}


There exists exists 2 problems of correlations in the output delivered by a MC. 

\begin{itemize}
	\item \textbf{Autocorrelation} is the
	\item \textbf{Cross-correlation}
\end{itemize}

As the dependence becomes stronger, the run length n must be larger in order
to achieve the same precision. Dependence exists both within the output for a single parameter
(autocorrelations) and across parameters (cross-correlations), we discuss this issue in section \hyperref[label]{text}.

\section{Posterior Predictive}
notation for posterior ? pi or f


As discussed in \hyperref[it2bayes]{item \textbf{2}} above, prediction is of important interest in EVT, and this is "facilitated" in the Bayesian paradigm. This also permits a more straightforward quantification of the inferential uncertainty associated.

\begin{definition}[Posterior Predictive density]
	Let $\tilde{X}$ denotes a future observation (e.g. $n+1$) with density $f(\tilde{x}|\boldsymbol{\theta})$. We define the \textbf{posterior predictive density} of $\tilde{X}$ given $\boldsymbol{x}$ as 
	
	\begin{equation}\label{eq:ppd}
	\begin{aligned}
	f(\tilde{x}|\boldsymbol{x})
	= & \int_{\Theta}f(\tilde{x},\theta | \boldsymbol{x})\cdot d\theta=\int_{\Theta} f(\tilde{x}|\theta)\cdot \pi (\theta|\boldsymbol{x})\cdot d\theta
	\\ := & \ \ \mathbb{E}_{\theta|\boldsymbol{x}}\big[f(\tilde{x}|\theta)\big]
	\end{aligned}
	\end{equation}
	where $\boldsymbol{x}$ is the vector of recorded annual maxima over $n$ years.	
\end{definition}
The second line of (\ref{eq:ppd}) emphasizes that we can evaluate $f(\tilde{x}|\boldsymbol{x})$ by averaging over all possible parameter values.
Uncertainty is reflected through $\pi(\theta|\boldsymbol{x})$ and the uncertainty due to variability in future observations through $f(\tilde{x}|\theta)$.

\begin{definition}[Posterior Predictive probability]
	The \textbf{posterior predictive probability} of $\tilde{X}$ exceeding some threshold $x$ is given by
	\begin{equation}
	\begin{aligned}
	\text{\emph{Pr}}\{\tilde{X}>x\ | \ \boldsymbol{x}\}= &\int_{\Theta}\text{\emph{Pr}}\{\tilde{X}>x \ | \ \theta \} \cdot \pi(\theta|\boldsymbol{x})\cdot d\theta \\ 
	= & \ \mathbb{E}_{\theta|\boldsymbol{x}}\big[\text{\emph{Pr}}(\tilde{X}>x \ | \ \theta)\big]
	\end{aligned}
	\end{equation}
\end{definition}
This quantity is often of interest in EVT as we are rather concerned with the probability of future unknown observable exceeding some threshold.

However, this quantity is difficult to obtain analytically. Hence, we will more rely on simulated approximations. Given a sample $\theta_1,\dots,\theta_r$ from the posterior $\pi(\theta|\boldsymbol{x})$, we use 

\begin{equation}
\text{Pr}\{\tilde{X}>x\ | \ \boldsymbol{x}\}\approx r^{-1}\sum_{i=1}^r\text{Pr}\{\tilde{X}>x \ | \ \theta_i\},
\end{equation}
where $\text{Pr}\{\tilde{X}>x \ | \ \theta_i\}$ follows directly from $f(x|\theta)$. 

We will now analyse more in-depth the numerical computations in the Bayesian paradigm or how we can get numerically a sample of the posterior distribution.


\subsection{Model Comparison}\label{sec:modcompbay}

pp.54 extremes in a climate change 

In a nonstationary GEV context, we would like to compare models as done in the frequentist setting in \hyperref[sec:gev_nonstatio]{Chapter \textbf{\ref{sec:gev_nonstatio}}} with the deviance statistic, the BIC, etc.
There may exist different nonstationary models for the location or for the scale, and we would like to assess which model is the most relevant to describe the data. We will only focus on parametric models such as those studied with tools in \hyperref[sec:gev_nonstatio]{Chapter \textbf{\ref{sec:gev_nonstatio}}}. 

Le assume that $\big\{M_1,\ldots,M_q\big\}$ is a set of $q$ candidate models for describing the data $\boldsymbol{x}$. Each $M_i$ uses a parameter vector $\theta^{M_j}$ whose dimension may differ across models. Let $\text{Pr}(M_1),\ldots,p(M_q)$ denote prior probabilities assigned to each model such that $\sum_{i=1}^q \text{Pr}(M_i)=1$.
It is then possible to compute models' posterior probabilities given $\boldsymbol{x}$ from the Bayes' Theorem applied in its discrete form : 

\begin{equation}\label{eq:modpost}
\text{Pr}(M_i|\boldsymbol{x})=\frac{\text{Pr}(\boldsymbol{x}|M_i)\cdot \text{Pr}(M_i)}{\sum_{j=1}^q \text{Pr}(\boldsymbol{x}|M_j)\cdot \text{Pr}(M_j)},
\end{equation}
where $\text{Pr}(\boldsymbol{x}|M_i)$ is the marginal likelihood of observations an can be computed as follows : 

\begin{equation*}
\text{Pr}(\boldsymbol{x}|M_i)=\int \text{Pr}(\boldsymbol{x}|\theta^{M_i},M_i)\cdot \text{Pr}(\theta^{M_i}|M_i)\cdot d\theta^{M_i}.
\end{equation*}
However, computing the marginal likelihood is difficult and suffers from the curse of dimensionality. \citet{Bos2002} provides various methods to compute the marginal likelihood when the size of the parameter vector is moderate.

Model posterior probabilities (\ref{eq:modpost}) can be used for model comparisons and predictions.

\subsubsection*{The Bayes Factor}

One approach is to use the \emph{Bayes factor}
to compare models. IT provides a measurement of the relevance of one model compared to anaother. For two models $M_i$ and $M_j$, it is defined as follows : 

\begin{equation}
B_{i,j}=\frac{\text{Pr}(M_i|\boldsymbol{x})}{\text{Pr}(M_j|\boldsymbol{x})}\cdot \Bigg(\frac{\text{Pr}(M_i)}{\text{Pr}(M_j)}\Bigg)^{-1}= \frac{\text{Pr}(\boldsymbol{x}|M_i)}{\text{Pr}(\boldsymbol{x}|M_j)}
\end{equation}
High values of $B_{i,j}$ gives stronger confidence in model $M_i$. Formal guidelines have been developed (see e.g. \citet{robert_1995}). It is also possible to compare several models (e.g. stationary) with other several models (e.g. nonstationary) via a composite Bayes factor.


\subsubsection*{Bayesian Model Averaging}

It is also possible to perform multi-model predictions by computing a weighted average of individual model predictions weighted by the posterior model probabilities. 
\emph{Bayesian model averaging} (BMA) accounts for model uncertainty. Interesting similarities with bagging, method discussed in \hyperref[sec:bagg]{Chapter \textbf{\ref{sec:bagg}}} which makes an average of bootstrapped models to improve the prediction accuracy, can be found. Both relies on the same data but BMA will use different models, and hence correlations between the predictions of the different models will be important. It is particularly useful when distinct models provide an acceptable description of the data, but yield different predictions.



\section{Bayesian Predictive Accuracy for Model Validation}



\subsection{Cross-validation for predictive accuracy}

When having large amount of data, we can use a well-known and widely used technique coming from Machine Learning. That is, dividing the dataset between a training (typically $75\%$ of the whole set) and a test set containing the remaining observations.
For example, having $N$ draws $\theta^{(1)},\theta^{(2)},\dots,\theta^{(N)}$ coming from the posterior $\pi(\theta|x_{train})$, we can score each value using (?)


\begin{equation}
\log\bigg[N^{-1}\sum_{t(i)?=1}^N f(x^*|\theta^{(t)})\bigg].
\end{equation}

However, we often do not have large amounts of data. Henceforth, we can use the \emph{cross-validation} technique which is more relevant in smaller dataset, but which is computationally more demanding. There exists several variants of them. 

\subsubsection*{Leave-one-out cross-validation}

The \emph{Leave-One-Out} (LOO) cross-validation is the 


\subsubsection*{$K$-fold cross-validation}

\citet{Vehtari_practical_2016}


Or we can use other criteria which avoid the computations. The basic approach is to use  $\log f(x|\bar{\theta})-p^*$ for $N$ draws $\theta^{(1)},\dots,\theta^{(N)}$ from $\pi(\theta|x)$. where $p^*$ represents the effective number of parameters and $\bar{\theta}$ the posterior mean. Several methods exists using this idea. We will see the two most important.

\subsubsection*{Deviance Information Criterion}

The \emph{Deviance Information Criterion}
(DIC) was first used by \citet{Spiegelhalter_bayesian_2002} and use the following estimate for the effective number of parameters 

\begin{equation}
p^*=2\cdot \bigg(\log f(x|\bar{\theta})-N^{-1}\sum_{t=1}^N\log f(x|\theta^{(t)})\bigg)
\end{equation}

It is defined on the deviance scale and smaller DIC values indicate better models.

\begin{equation}
\text{DIC}= 2\log f(x|\bar{\theta}) - \frac{4}{N} \sum_{t=1}^N\log f(x|\theta{(t)})
\end{equation}



\subsubsection*{Widely Applicable Information Criterion}

The \emph{Widely Applicable Information Criterion} (WAIC) is a more recent approach proposed by \citet{Watanabe_asymptotic_2010} and is given by 


\begin{equation}
\text{WAIC}= 2 \sum_{i=1}^n\big[\log\big\{\mathbb{E}_{\theta|x}f(x_i|\theta)\big\}\big]- \mathbb{E}_{\theta|x}\log f(x_i|\theta) 
\end{equation}

or 

\begin{equation}
\text{WAIC} = \sum_{i=1}^n\Bigg[2\log \bigg(N^{-1}\sum_{t=1}^N f(x_i|\theta^{(t)})\bigg)-\frac{4}{N}\sum_{t=1}^N\log f(x_i|\theta^{(t)})\Bigg]
\end{equation}



There exists for sure other several methods, as proposed by \citet{gelman_understanding_2014}.
 
 'LOO and WAIC have various advantages over simpler estimates of predictive error such asAIC and DIC but are less used in practice because they involve additional computational steps' 


For each generated chains with dispersed starting values, we evaluate separately the information criteria. The discrepancies between the chains are small (?), which is a good sign. 

\section{Bayesien Inference ?}

\subsection{Bayesian Credible Intervals}\label{bayes_cred_int}

The Bayesian \emph{credible intervals} are inherently different from the frequentist's confidence intervals. In the Bayesian intervals, the bounds are treated as fixed and the estimated parameter as a random variable, while in the frequentist's setting, bound are random variables and the parameter is a fixed value.

There exist mainly two kinds of credible interval in the Bayesian sphere : 

\begin{itemize}
	\item The \emph{Highest Probability Interval} (HPD) which is defined as the shortest interval containing $x\%$ of the posterior probability, e.g. if we want a $95\%$ HPD interval $(\xi_0,\xi_1)$ for $\xi$ :
	
	\begin{equation} 
	\int_{\xi_0}^{\xi_1}\pi(\xi|\boldsymbol{x})d\xi=0.95 \qquad\text{with}\qquad \pi(\xi_0|\boldsymbol{x})=\pi(\xi_1|\boldsymbol{x}).
	\end{equation}
	
	It is often the preferred interval as it gives the parameter's values having the highest posterior probability. 
	\item The Quantile-based credible intervals or \emph{equal-tailed interval} picks an interval which ensures a probability of being below this interval as likely as of being above it. 
	For some posterior distribution which are not symmetric, this could be misleading, thus it is not the most recommended interval. (see ..)
	However, these are often easily obtained when we have a random smaple of the posterior...(?)
\end{itemize}
For an asymmetric distribution,
the HPD interval can be a more reasonable summary than the central probability
interval ( see illustration ...). For symmetric densities, HPD and central interals are the same while HPD is shorter for asymmetric densities.
See \citet{liu_simulation-efficient_2015}....

\subsection{Distribution of Quantiles : Return Levels}

The Markov chains generated can be transformed to estimate quantities of interest such as quantiles and hence return levels.

The values can be retrieved in the same manner as we have done in the GEV frequentist setting in (\ref{rleqgev}). If the df $F$ associated is GEV then $y_m = -\log(1-m^{-1})$, and if $F$ is GPD then $y_m=m^{-1}$.

$r_m$ is the quantile corresponding to the upper tail probability $p=m^{-1}$. 

We can use the values of the samples generated by the posterior to estimate features of this distribution. (... see edbayes)
 
 

%\section{Bayesian Model Averaging}


%\section{Bayesian Neural Networks}

