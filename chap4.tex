\section{Preliminaries : Motivations}\label{sec:bayprelim}


This chapter will rely on 	 multidimensional set of parameters $\theta$ of a GEV model, i.e. $\theta'=(\mu,\sigma,\xi)$ in a stationary context, and where $\nu=\log \sigma$ is often used instead of $\sigma$ to get rid of its positivity constraint.
 Unless stated differently, we will write $\theta$ as this $3$D vector to facilitate readability.
 
We already argued that likelihood-based methods have desirable properties for inference, and hence we have adopted in \hyperref[sec::1]{Chapter \textbf{\ref{sec::1}}} and \textbf{\ref{sec::3}} the methods of (generalized or penalized) maximum likelihood to compute our estimators. Bayesian inference introduces a flexible alternative that also relies on the likelihood. First, we present the most fundamental result for Bayesian inference which is a direct interpretation of the Bayes' Theorem.

\begin{definition}[Posterior distribution]
	Let $\boldsymbol{x}=(x_1,\dots,x_n)$
	denote the vector of $n$ independent observations of a random variable $X$ with density $f(x|\theta)$, and let $\pi(\theta)$ denote the density of the prior distribution for $\theta$. According to Bayes' theorem, the \textbf{posterior distribution} of $\theta$ is 
	\begin{equation}\label{bayeseq}
	\pi (\theta|\boldsymbol{x})=\frac{\pi(\theta)\cdot L(\theta|\boldsymbol{x})}{\int_{\Theta} 
		\pi(\theta)\cdot L(\theta|\boldsymbol{x}) \cdot d\theta}\propto \pi(\theta)\cdot 
	L(\theta|\boldsymbol{x}),
	\end{equation}
	where $L(\theta|\boldsymbol{x})=\prod_i f(x_i|\theta)$ is the likelihood since the $x_i$'s are independent.
\end{definition}
This result (\ref{bayeseq}) provides the probabilistic framework to convert an initial set of beliefs about $\theta$, represented by the prior $\pi(\theta)$, into a posterior distribution that will be convenient to use for inference.
Relying on the posterior distribution to compute an estimate $\hat{\theta}$ of $\theta$ provides interesting characteristics :
\begin{itemize}
	\item Whenever possible, it allows to introduce an other source of knowledge coming from the domain, by the elicitation of a prior (see \hyperref[sec:prior]{Section \textbf{\ref{sec:prior}}}). However, it also introduces subjectiveness and antagonists contend that, since different analysts would specify
	different priors, all conclusions become meaninglessly subjective.
	
	\item\label{it2bayes} Accounting for  uncertainty is handled most easily in the
	Bayesian paradigm relying on distribution's properties of $\pi(\theta|\boldsymbol{x})$, for example its variance. Now, the parameters are treated as random variables. %\cite[pp.106]{dey_extreme_2016}
	 It also permits an elegant way of making future predictions, which is the most important issue in EVT.
	
	\item Bayesian framework can overcome the regularity conditions of the likelihood inference
	(see \hyperref[likgevintro]{Section \textbf{\ref{likgevintro}}}) by providing a viable alternative in cases when ML breaks down. Actually, we will see that the annual maximum temperatures we will consider are not far from the problematic cases ($\xi<-0.5$). 
	Moreover, Bayesian confidence intervals (i.e. \emph{credible} intervals, see \hyperref[bayes_cred_int]{Section \textbf{\ref{bayes_cred_int}}}) require no more need to fall to asymptotic theory.
\end{itemize}
In EVT, Bayesian inference is a wide expanding domain that has nearly infinite possibilities.
With strong probabilistic foundations, this method now mostly relies on computational capabilities (see \hyperref[sec:baymcmc]{Section \textbf{\ref{sec:baymcmc}}}).


\section{Prior Elicitation}\label{sec:prior}


Sometimes viewed as its greatest strength of Bayesian inference from the amount of information that can be retrieved, and sometimes viewed as its main pitfall due to the unquantifiable subjectivity that is introduced, the construction of the prior is at the center of Bayesian analysis.
Priors are necessary in the Bayesian paradigm to compute (\ref{bayeseq}). It requires the legitimate statement of a domain's expert, to make this viewed as objective.

Priors may not be of great importance if the number $n$ of observations is large. From (\ref{bayeseq}) we see that the amount of information contained in the data through $L(\theta|\boldsymbol{x})$ will be prominent compared to this contained in the prior $\pi(\theta)$.
However, this configuration is rare in EVT where we are dealing with small constructed datasets. For this reason, incorporating external information through the prior should be (rigorously) studied. This type of prior is thus called \emph{informative priors}, while those who give most weight to data are called \emph{non-informative} (or \emph{objective}) \emph{priors}. We focus on priors that are widely applied in EVT.




\subsection{Trivariate Normal Distribution}\label{sec:trivnorm}

The trivariate normal prior distribution on $\theta'=(\mu, \nu,\xi)$ leads to the following prior density :

\begin{equation}
\pi(\theta) \propto \sigma^{-1}\cdot \exp\bigg\{-\frac{1}{2}(\theta'-\boldsymbol{m})'\Sigma^{-1}(\theta'-\boldsymbol{m})\bigg\},
\end{equation}
where the mean vector $\boldsymbol{m}$ and the symmetric positive definite $[3\times 3]$ covariance matrix $\Sigma$ must be specified. This approach was used by \citet{coles_1996_bay} but other parametrizations exist. For example, changing $\mu$ to $\log \mu$ can be useful if a physical lower bound for this parameter must be specified.


\subsection{Gamma Distributions for Quantile Differences}

This method constructs priors on the quantile space, for fixed probabilities. Let $\text{Pr}(X>q_p)=p$, where $X$ has a GEV distribution (\ref{gevgen}). Then,

\begin{equation*}
q_p=\mu + \sigma\cdot\xi^{-1}\cdot\big(x_p^{-\xi}-1\big),
\end{equation*}
where $x_p=-\log (1-p)$. Indeed, we notice that it is a $m$-return level (\ref{rleqgev}) with $p=m^{-1}$. The prior distribution is constructed in terms of the quantiles $(q_{p_1},q_{p_2}, q_{p_3})$ for specified probabilities $p_1>p_2>p_3$. It is easier to work with the differences $(\tilde{q}_{p_1},\tilde{q}_{p_2}, \tilde{q}_{p_3})$, with $\tilde{q}_{p_i}-\tilde{q}_{p_{i-1}}$, $i=1,2,3$, where $q_{p_0}$ is the physical lower endpoint of the process variable. We can take priors on the quantile differences to be independent with 

\begin{equation*}
\tilde{q}_{p_i}\sim \text{Gamma}(\alpha_i,\beta_i), \qquad\quad \alpha_i,\beta_i>0, \quad \text{for} \ \  i=1,2,3.
\end{equation*}
The differences $(\tilde{q}_{p_2},\tilde{q}_{p_3})$ only depend on $(\sigma,\xi)$. Therefore, prior information on $\mu$ arises only through $\tilde{q}_{p_1}$. Hyperparameters $(\alpha_1,\alpha_2,\alpha_3)$ and $(\beta_1,\beta_2,\beta_2)$ and probabilities $p_1>_2>p_3$ must all be specified. Constructed by \citet{coles_tawn_1996}, this leads to 

\begin{equation}
\pi(\theta)\propto J(\theta)\prod_{i=1}^3 \tilde{q}_{p_i}^{\alpha_i-1}\exp\big\{-\tilde{q}_{p_i}\cdot \beta_i^{-1}\big\}, \qquad \ q_{p_1}<q_{p_2}<q_{p_3},
\end{equation}
where $J(\theta)$ is the Jacobian of the transformation from $(q_{p_1},q_{p_2},q_{p_3})$ to $\theta$, i.e.

\begin{equation}
J(\theta)=\sigma\cdot\xi^{-2}\cdot\abs*{\sum_{\substack{i,j\in(1,2,3) \\ i<j}} (-1)^{i+j} (x_ix_j)^{-\xi} \log\big\{x_j\cdot x_i^{-1}\big\}}
\end{equation}
where $x_i=-\log(1-p_i)$ for $i=1,2,3$.

\subsection{Beta Distributions for Probability Ratios}

This method does the opposite of the last one. It constructs priors on the
probability space, for fixed quantiles.
 Let $\text{Pr}(X>q)=p_q$, where $X\sim \text{GEV}(\theta)$ with $\theta=(\mu,\sigma,\xi)$. Then, 

\begin{equation*}
p_q=1-\exp\bigg\{-\Big[1+\xi\cdot\sigma^{-1}(q-\mu)\Big]_+^{-\xi^{-1}}\bigg\}.
\end{equation*}
A prior can be defined in terms of $(p_{q_1},p_{q_2},p_{q_3})$ for $q_1<q_2<q_3$, with $p_{q_0}=1$ and $p_{q_4}=0$. Since $p_{q_1}>p_{q_2}>p_{q_3}$ it easier to work with $(\tilde{p}_{q_1},\tilde{p}_{q_2},\tilde{p}_{q_3})$, where $\tilde{p}_{q_i}=p_{q_i}/p_{q_{i-1}}$ for $i=1,2,3$. We can take 

\begin{equation}
\tilde{p}_{q_i}\stackrel{\independent}{\sim}\text{Beta}\Bigg(\sum_{j=i+1}^4 \alpha_j,\alpha_i\Bigg), \qquad \ i=1,2,3.
\end{equation}
where $(\alpha_1,\alpha_2,\alpha_3,\alpha_4)$ are positive hyperparameters.
This construction from \citet{Crowder1992} leads to 

\begin{equation*}
\pi(\theta)\propto J(\theta)\prod_{i=1}^4(p_{q_{i-1}}-p_{q_i})^{\alpha_i-1}, \quad  p_{q_1}>p_{q_2}>p_{q_3} \ \ \ \text{and} \ \ 1+\xi\sigma^{-1}(q_i-\mu)>0 \ \forall i=1,2,3.
\end{equation*}
$J(\theta)$ is the Jacobian of the transformation from $(p_{q_1}, p_{q_2},p_{q_3})$ to $\theta$. It can be shown that 

\begin{equation}
J(\theta)=\sigma\cdot\xi^{-2}\cdot\prod_{i=1}^3g(q_i)\cdot \abs*{\sum_{\substack{i,j\in(1,2,3) \\ i<j}} (-1)^{i+j} (x_ix_j)^{-\xi} \log\big\{x_j\cdot x_i^{-1}\big\}},
\end{equation}
where $x_i=-\log(1-p_{q_i})$ for $i=1,2,3$ and $g(q_i)=x_i^{1+\xi}\cdot e^{-x_i}\cdot\sigma^{-1}$
is the density of the GEV.
Specific example to construct this prior in practice can be found in \citet[pp.272]{dey_extreme_2016}.

\subsection{Non-informative Priors}\label{sec:noninfoprior}

Receive a correct and accepted advice from an expert is often difficult.
So, in many cases, it is not possible to inject information through the prior. 
Hence, priors must be constructed to represent this lack of knowledge and not influence posterior inferences. 

Parameters of the prior distribution are often called \emph{tuning parameters} or \emph{hyperparameters} to emphasize that we can tune the amount of information provided through the prior. In practical applications with  Markov Chains (see \hyperref[sec:baymcmc]{Section \textbf{\ref{sec:baymcmc}}}), these values are often tuned to who maximize the convergence of the posterior to its stationary target distribution. However, adjustments of these priors must always be thought in practical applications since objectives can be multiple. For example, if we take $\pi(\theta)\sim\mathcal{N}(\mu,\sigma^2)$,  the hyperparameter $\sigma^2$ will be tuned to represent the amount of knowledge we want to incorporate. It will tend to infinity to be non-informative (vague prior). However, note that it can be a huge help with computation to have less diffuse priors, even
if they're not informative enough to have a noticeable impact on the posterior.


There exists a vast amount of uninformative priors in the literature (see e.g. \cite{yang_catalog_1996}, \cite{ni_noninformative_2003}, etc).
This family of priors can be \emph{improper}, i.e. priors for which the integral of $\pi(\theta)$ over the parameter space is not finite, but it is only valid if the posterior target distribution is proper. 


\subsubsection*{Jeffrey's prior} 
Discovered by \citet{Jeffreys61a}, this prior is specified as 

\begin{equation}
\pi(\theta)\propto \sqrt{\det I(\theta)}, \qquad \text{where }\quad I_{ij}(\theta)=\mathbb{E}_{\theta}\Bigg[-\frac{\partial^2\log f(X|\theta)}{\partial\theta_i\partial\theta_j}\Bigg]; \ \ i,j= 1,\dots,d.
\end{equation}
%where $f(\boldsymbol{x}|\theta)$ is the density function of X.
This prior is a standard starting rule for an objective analysis.
This prior is invariant to reparametrization, but has a complex form for GEV models, and it exists only when $\xi>-0.5$ in GEV models, where it is function of $\xi$ and $\sigma$ only.


\subsubsection*{Maximal Data Information  prior}
Maximal Data Information (MDI) priors are defined to provide maximal average data information on $\theta$. These are not invariant under reparametrization but are easy to implement. For GEV models, it is defined as 

\begin{equation*}
\pi(\theta)= \exp\Big\{ \mathbb{E}\big[\log f(X|\theta
)\big]\Big\} \propto \sigma^{-1}\exp\Big\{-\gamma(1)\cdot (1+\xi)\Big\}.
\end{equation*}
where $\gamma(1)$ denotes the Euler's constant.
However, it has been showed by \cite{northrop_posterior_2016} that 
both Jeffrey and MDI priors give improper posterior when there are no truncation of the shape parameter, and hence we must restrict the fact that $\pi(\theta)\rightarrow\infty$ as $\xi\rightarrow(\textcolor{red}{-})\infty$ for Jeffreys (\textcolor{red}{MDI}), in order to obtain a proper posterior.

\subsubsection*{Vague priors}
The preferred alternative is often to construct uninformative priors by using proper priors which are near flat, e.g. which are uniform or which exhibits very large variance.
In GEV, we will often take independent normally distributed priors each with a large (tuned) variance. When these variances increase, we get at the limit

\begin{equation}
\pi(\theta)=\pi(\mu,\nu,\xi)\stackrel{(\independent)}{=}\pi(\mu)\cdot\pi(\nu)\cdot\pi(\xi)\ \propto 1,
\end{equation}
where $\nu= \log\sigma$.
Taking a trivariate normal distribution as prior as in \hyperref[sec:trivnorm]{Section \textbf{\ref{sec:trivnorm}}} is often difficult as it involves 9 hyperparameters in total ($\mu$, $\xi$ and $7$ in $\Sigma$).




\section{Bayesian Computation : Markov Chains}\label{sec:baymcmc}

The main difficulty of Bayesian inference has been the computation of the normalizing integral in the denominator of (\ref{bayeseq}), which is often intractable. The prior $\pi(\theta)$ could be chosen so that the integral could be evaluated analytically, but this is rather exceptional. Moreover, it is not possible in EV models since they involve distributions that are not within the exponential family. 
More complex cases where the likelihood $L(\theta|\boldsymbol{x})$ is intractable can be dealt with approximate Bayesian computation (\citet{Beaumont2025}), but we will not consider this subject in this text.

Hence, simulation-based methods have been developed to sample from an arbitrary posterior distributions $\pi(\theta|\boldsymbol{x})$. Simulations of $N$ iid samples $\theta_1,\theta_2,\dots,\theta_N$ from $\pi(\theta|\boldsymbol{x})$ can be used to estimate features of interest.
But simulating from $\pi(\theta|\boldsymbol{x})$ is usually not achievable and we need \emph{Markov Chain Monte Carlo} (MCMC) techniques. 


\begin{definition}[\emph{First-order discrete-time} Markov Property]
	Let $k_0,k_1,\dots$ be the states associated to a sequence of time-homogeneous random variables, say $\big\{\theta_t:t\in\mathbb{N}\big\}$.
	The Markov property states that the distribution of the future state $\theta_{t+1}$ depends only on the distribution of the current state $\theta_{t}$,  i.e.
	
	\begin{equation}
	\text{\emph{Pr}}\big\{\theta_{t+1}=k_{t+1}\ |\ \theta_t=k_t,\ \theta_{t-1}=k_{t-1},\dots\big\} = \text{\emph{Pr}}\big\{\theta_{t+1}=k_{t+1}\ | \ \theta_t=k_t\big\}.
	\end{equation}
	In other words, given $\theta_{t}$, we have that $\theta_{t+1}$ is independent of all the states prior to $t$.
\end{definition} 
This in one of the most important results that defines MCMC techniques (see \citet[section 2.2.3]{angelino_patterns_2016} for more results).
We use this techniques it to simulate a Markov chain $\theta_1,\theta_2,\dots,\theta_N$ that converges to the target distribution $\pi(\theta|\boldsymbol{x})$.
This means that, after some \emph{burn-in period} $B$, the chain $\theta_{B+1},\theta_{B+2},\ldots,\theta_N$ can be treated as random sample from $\pi(\theta|\boldsymbol{x})$ that has reached its target \textbf{stationary} distribution. 
This will be achieved if the so-generated chain is

\begin{enumerate}
	\item \emph{aperiodic}, i.e. it does not have period $\eta$ for any $\eta>1$.
	\item \emph{irreducible} or \emph{ergodic}, i.e. we can get from any state to any other states (possibly in several steps).
\end{enumerate}




\subsection{Metropolis–Hastings}

Named after \citet{metropolis_1953} and  \cite{hastings_monte_1970} , the \emph{Metropolis–Hastings} algorithm is the pioneering MCMC algorithm for Bayesian analysis. It is a conceptually simply method with specific update proposals that are accepted or rejected according to an appropriate rule.
Details on the algorithm are left in \hyperref[app:mh]{Appendix \textbf{\ref{app:mh}}}.
We can summarize the \emph{pros} and \emph{cons} of this algorithm :

\begin{itemize}
	\item \emph{PROS} : Very easy to program and works even for relatively complex densities.
	\item \emph{CONS} : Can be very inefficient, in the sense that it will require lots of iterations before the stationary target distribution will be reached. 
\end{itemize}
This algorithm requires some tuning through the proposal distribution. 
It is recommended to target an acceptance rate of $\approx 0.25$ since all components of $\theta$ are updated simultaneously, in order to facilitate convergence, following \citet{BEDARD20082198}.
The idea is that if the variance of the proposal distribution is too large, most proposals will be rejected,	i.e. jumps through the chain will be too large, and the converse if the variance of the proposal distribution is too small.
Both are harmful for the objective of an efficient visit of the whole parameter space. 


\subsection{Gibbs Sampler}

Originated from \citet{Geman_1984}, the \emph{Gibbs Sampler} can be seen as a special case of the MH algorithm.
Suppose our parametric vector $\theta$ can be divided into $d$ subvectors $(\theta_1,\dots,\theta_d)$, and suppose that each of these subvectors represent a single parameter, i.e. $(\mu,\sigma,\xi)$ for the stationary case, so that $d=3$ in this model.
For each $t=1,\dots,N$, the Gibbs sampler samples the subvectors $\theta_t^{(j)}$ conditionally on both the data $\boldsymbol{x}$ and the remaining subvectors at their current values.
More details on the algorithm are left in \hyperref[app:gibbs]{Appendix \textbf{\ref{app:gibbs}}}.

It is recommended to target an acceptance rate of $\approx 0.25$ when all components of $\theta$ are updated simultaneously, and $\approx 0.45$ when the components are updated one at a time, that is when $\theta_j$ is univariate (see e.g. \citet[chap.11]{gelman_bayesian_2013}), so that the so-generated Markov-chain has desirable properties.

We must also note the increase of complexity of this sampler compared to the Metropolis-Hastings, since the nested loop in Algorithm \ref{algo:gib} implies that there are $d$ iterations with each simulation. Finally, we summarize the
\emph{pros} and \emph{cons} of this sampler : 

\begin{itemize}
	\item \emph{PROS} : Easy to program and, for some problems, it can be very efficient. Pleasant way to split multidimensional problems into simpler (i.e. typically univariate) densities.
	\item \emph{CONS} : Sometimes hard to compute the conditional distributions. Not all densities can be split into pleasant conditionals equations.
\end{itemize}
Note that it is possible to combine Gibbs sampling with some Metropolis steps as in \cite[chap.11]{Gelman95bayesiandata}, when a conditional distribution cannot be identified.


\subsection{Hamiltonian Monte Carlo}

\citet{betancourt_diagnosing_2016} emphasizes that the performance of the  MCMC depends on  how effectively the Markov transition guides the Markov chain along the neighborhoods of high probability. If the exploration is slow, then the estimators will become computationally inefficient, and if the exploration is incomplete then the estimators will become biased. Moreover, \citet{betancourt_2017_con} said "guess-and-check strategy of Random Walk Metropolis is doomed to fail in
high dimensional spaces where there are an exponential number of directions
in which to guess but only a singular number of directions that stay within the
typical set and pass the check ". This is a \emph{curse of dimensionality} issue since there are much more volume outside any given neighborhood than inside of it.
In an EV framework where the sample size is limited and the dimension can easily increase with nonstationary models, we think necessary to consider other form of sampling to address computational efficiency. \cite{hartmann_bayesian_2016} have demonstrated in a GEV application that HMC  (and Riemann manifold HMC) are more computationally efficient than traditional MCMC algorithms such as MH. 


Following \citet[chap.33]{stan_development_team_stan_2012}, the \emph{Hamiltonian Monte Carlo} (HMC)
algorithm starts at specified or random initial values for $\theta$. Then, it uses derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration. 
Finally, a Metropolis acceptance step is applied, and a decision is made whether to update to the new state or keep the existing state. 
HMC permits to better exploit properties of
the target distribution to make informed jumps through neighborhoods of high probability, while avoiding neighborhoods of low probability.  We leave interesting details of this algorithm in \hyperref[app:hmc]{Appendix \textbf{\ref{app:hmc}}}.

However, this new MC method is build from differential geometry and topics such as \emph{Riemannian manifolds}, \emph{microcanonical Disintegration},\emph{ kinetic energy}, etc. Hence, we aim not at theoretically explaining all this algorithm. Moreover, lots of efforts have been spent, but we faced a difficulty when building the (nonstationary) GEV model with HMC in \href{http://mc-stan.org/}{STAN}\footnote{High level probabilistic programming language which incorporates HMC with Nuts.}. We would like to point out that this is probably due to a problem of misspelled constraints in the boundaries of the GEV distribution, but it is still an open issue... More information will come in \hyperref[sec:anabayes]{Section \textbf{\ref{sec:anabayes}}}.

We summarize the \emph{pros} and \emph{cons} of this algorithm used within STAN : 
\begin{itemize}
	\item \emph{PROS} : Easy to program as we just have to write down the model. Very efficient in general, and works for all types of problems.
	\item \emph{CONS} : Need to learn how to use STAN, very difficult to understand the theory behind the mechanism, less control over the sampler (but maybe it is for the best?).
\end{itemize}
Pros and cons that are specific to compiled languages must also be noticed.


\section{Convergence Diagnostics}\label{sec:convbay}

When applying MCMC algorithms to estimate posterior distributions, it is vital to assess convergence of the algorithm to convince ourselves that we reached the stationary target distribution. We enumerate some of the key steps we must keep in mind when thinking about convergence, and hence obtaining reliable results.

\begin{enumerate}
	\item A sufficient \emph{burn-in period} $B<N$ must be chosen to ensure that the convergence to the stationary posterior distribution has occurred, as it will vanish the influence of starting values. For the same reason, a sufficient number of simulations $N$ is required to get accurate MCMC estimates.
	\item Several dispersed starting values must have been simulated to ensure all the regions of high probability of the parameter space have been explored. This is particularly important when the target distribution is complex. The chains must then have good mixing properties. A common technique is to run different chains several times with different starting values, and then combine a proportion of each chain (typically $50\%$) to get the final chain. %This procedure wants to ensure a proper mixing behaviour. 
	\item A good choice of the proposal distribution is crucial since a poorly chosen may drastically reduce the speed of the algorithm. To a certain extent, the proposal should be similar to the target distribution. Gaussian proposals have our preference for their convenience and are "easy-to-tune", but note that fat-tailed distributions could be interesting since they will occasionally generate a candidate far from the previous one, reducing the risk of being stuck in a local mode.
	\item The \emph{autocorrelations} within single parameter's chains must be small to keep a good effective sample size.
    This is the same for \emph{cross-correlations}, i.e. the correlations between the chains of different parameters in the vector $\theta$.  Both impacts the speed of convergence of the algorithm.
\end{enumerate}
Some important diagnostics are defined in \hyperref[app:convdiag]{Appendix \textbf{\ref{app:convdiag}}}. We must keep in mind that no convergence diagnostics can prove that convergence really happened and validate the results.
However, a combined use of several relevant diagnostics will be required to increase our confidence that convergence actually happened.



\section{Bayesian Inference}\label{sec:bayinf}

Assuming the posterior has been correctly explored using a MCMC algorithm, leaving us with $N$ realizations $\theta_1,\ldots,\theta_N$ from the posterior. Since $\theta$ is $d$-variate, marginal estimates are simply obtained by considering the realizations $\theta_1^{(i)},\ldots,\theta_N^{(i)}$  from the marginal posterior of the parameter $\theta^{(i)}$. It is possible to obtain point-estimates from the posterior, for example by computing the marginal mean or other quantiles from  $\theta_1^{(i)},\ldots,\theta_N^{(i)}$. It is also possible to derive a modal estimate by selecting the parameter vector corresponding to the largest posterior value.

\subsection{Distribution of Quantiles : Return Levels}

The Markov chains generated can be transformed to estimate quantities of interest such as quantiles or return levels.
When $F$ is GEV, the distribution of quantiles can be retrieved in the same manner as in the frequentist setting in (\ref{rleqgev}) with $y_m = -\log(1-m^{-1})$. Let $r_m= q_p(\theta)$ and let $F(q_p)=1-p$. Then, for each $p$, the samples $\theta_1,\ldots,\theta_N$ can be substituted into (\ref{rleqgev}) to yield $q_p(\theta_1),\ldots,q_p(\theta_N)$. These values can be used to estimate features of the prior and posterior distributions of $q_p(\theta)$ in the same way that the values $\theta_1,\ldots,\theta_N$ were used to estimates features of $\pi(\theta|\boldsymbol{x})$. 


\subsection{Bayesian Credible Intervals}\label{bayes_cred_int}

The Bayesian \emph{credible intervals} are inherently different from the frequentist's confidence intervals. In the Bayesian intervals, the bounds are treated as fixed and the estimated parameter as a random variable, while in the frequentist's setting, bound are random variables and the parameter is a fixed value. A $95\%$ Bayesian credible intervals contains $95\%$ of the posterior probability for $\theta$.

There exist mainly two kinds of credible interval in the Bayesian framework : 

\begin{itemize}
	\item The \emph{quantile-based} credible intervals or \emph{equal-tailed interval} picks an interval to ensure that the probability of being below this interval is as likely as of being above it. 
	For some posterior distribution which are not symmetric, this could be misleading, and is then not recommended in this case. 
	The advantage is that they are easily obtained from a random sample of the posterior.
	\item The \emph{Highest Probability Interval} (HPD) which is defined as the shortest interval containing $x\%$ of the posterior probability, e.g. if we want a $95\%$ HPD interval $(\xi_0,\xi_1)$ for $\xi$, we calculate :
		
	\begin{equation} 
	\int_{\xi_0}^{\xi_1}\pi(\xi|\boldsymbol{x})\cdot d\xi=0.95 \qquad\text{with}\qquad \pi(\xi_0|\boldsymbol{x})=\pi(\xi_1|\boldsymbol{x}).
	\end{equation}
		
	This interval is often preferred as it gives the parameter's values having the highest posterior probabilities. 
		
\end{itemize}
For symmetric densities, HPD and central interals are the same while HPD is shorter for asymmetric densities, see e.g. illustrations in \citet{liu_simulation-efficient_2015}.


\section{Posterior Predictive Distribution}\label{sec:ppd}

Together with parameter estimation, prediction is the ultimate aim of inference in EVT, and it is neatly handled within the Bayesian paradigm. It permits a straightforward quantification of the uncertainty associated. It is made through the \emph{posterior predictive distribution}

\begin{definition}[Posterior Predictive Density]
	Let $\tilde{X}$ denotes a future observation (e.g. $n+1$) with density $f(\tilde{x}|\boldsymbol{\theta})$. We define the \textbf{posterior predictive density} of $\tilde{X}$ given $\boldsymbol{x}$ as 
	
	\begin{equation}\label{eq:ppd}
	\begin{aligned}
	f(\tilde{x}|\boldsymbol{x})
	= & \int_{\Theta}f(\tilde{x},\theta | \boldsymbol{x})\cdot d\theta=\int_{\Theta} f(\tilde{x}|\theta)\cdot \pi (\theta|\boldsymbol{x})\cdot d\theta
	\\ := & \ \ \mathbb{E}_{\theta|\boldsymbol{x}}\big[f(\tilde{x}|\theta)\big],
	\end{aligned}
	\end{equation}
	with $f(\tilde{x}|\theta,\boldsymbol{x})=f(\tilde{x}|\theta)$, where $\boldsymbol{x}$ is the $n$-dimensional vector of observations.
\end{definition}
The second line of (\ref{eq:ppd}) emphasizes that we can evaluate $f(\tilde{x}|\boldsymbol{x})$ by averaging over all possible parameter values.
Uncertainty in the model is reflected through $\pi(\theta|\boldsymbol{x})$, and uncertainty due to variability in future observations through $f(\tilde{x}|\theta)$.

\begin{definition}[Posterior Predictive Distribution]
	The \textbf{posterior predictive distribution} of a future observation $\tilde{X}$ is
	\begin{equation}
	\begin{aligned}
	\text{\emph{Pr}}\{\tilde{X}<x\ | \ \boldsymbol{x}\}= &\int_{\Theta}\text{\emph{Pr}}\{\tilde{X}<x \ | \ \theta \} \cdot \pi(\theta|\boldsymbol{x})\cdot d\theta \\ 
	= & \ \mathbb{E}_{\theta|\boldsymbol{x}}\big[\text{\emph{Pr}}(\tilde{X}<x \ | \ \theta)\big].
	\end{aligned}
	\end{equation}
\end{definition}
However, this quantity is difficult to obtain analytically. Hence, we will more rely on simulated approximations. Given a sample $\theta_1,\dots,\theta_r$ from the posterior $\pi(\theta|\boldsymbol{x})$, we use 

\begin{equation}
\text{Pr}\{\tilde{X}<x\ | \ \boldsymbol{x}\}\approx r^{-1}\sum_{i=1}^r\text{Pr}\{\tilde{X}<x \ | \ \theta_i\},
\end{equation}
where $\text{Pr}\{\tilde{X}<x \ | \ \theta_i\}$ follows directly from $f(x|\theta)$. 

%We will now analyse more in-depth the numerical computations in the Bayesian paradigm or how we can get numerically a sample of the posterior distribution.


\section{Model Comparison}\label{sec:modcompbay}

%pp.54 extremes in a climate change 

In a nonstationary GEV context, we would like to compare models in a similar way as in the frequentist setting in \hyperref[sec:gev_nonstatio]{Chapter \textbf{\ref{sec:gev_nonstatio}}} with the deviance statistic, the BIC, etc.
There may exist different nonstationary models for the location or for the scale, and we would like to assess which model is the most relevant to describe the data.
Although the posterior distribution quantifies estimation uncertainties, it remains conditional on the modeling assumptions. And as in any other estimation approach, it is a vital task to select the best possible model and validate it, otherwise the underlying inferences could be drastically different. 

Le assume that $M_1,\ldots,M_q$ is a set of $q$ candidate models to describe $\boldsymbol{x}$. Each $M_i$ uses a parameter vector $\theta^{M_j}$ whose dimension may differ across models. Let $\text{Pr}(M_1),\ldots,\text{Pr}(M_q)$ denote prior probabilities assigned to each model such that $\sum_{i=1}^q \text{Pr}(M_i)=1$.
It is then possible to compute models' posterior probabilities given the observations, from the discrete form of the Bayes' Theorem : 

\begin{equation}\label{eq:modpost}
\text{Pr}(M_i|\boldsymbol{x})=\frac{\text{Pr}(\boldsymbol{x}|M_i)\cdot \text{Pr}(M_i)}{\sum_{j=1}^q \text{Pr}(\boldsymbol{x}|M_j)\cdot \text{Pr}(M_j)},
\end{equation}
where $\text{Pr}(\boldsymbol{x}|M_i)$ is the marginal likelihood of observations an can be computed as follows : 

\begin{equation*}
\text{Pr}(\boldsymbol{x}|M_i)=\int \text{Pr}(\boldsymbol{x}|\theta^{M_i},M_i)\cdot \text{Pr}(\theta^{M_i}|M_i)\cdot d\theta^{M_i}.
\end{equation*}
However, computing the marginal likelihood is difficult and suffers from the curse of dimensionality. \citet{Bos2002} provides various methods to compute the marginal likelihood when the size of the parameter vector is moderate.
Model posterior probabilities (\ref{eq:modpost}) can be used for model comparisons and predictions.

\subsubsection*{The Bayes Factor}

One approach is to use the \emph{Bayes factor}
to compare models. It measures the relevance of one model compared to another. For two models $M_i$ and $M_j$, it is defined as : 

\begin{equation}
B_{i,j}=\frac{\text{Pr}(M_i|\boldsymbol{x})}{\text{Pr}(M_j|\boldsymbol{x})}\cdot \Bigg(\frac{\text{Pr}(M_i)}{\text{Pr}(M_j)}\Bigg)^{-1}= \frac{\text{Pr}(\boldsymbol{x}|M_i)}{\text{Pr}(\boldsymbol{x}|M_j)}.
\end{equation}
High values of $B_{i,j}$ give stronger confidence in model $M_i$. Formal guidelines have been developed (see e.g. \citet{robert_1995}). It is also possible to compare several models (e.g. stationary) with other several models (e.g. nonstationary) via a composite Bayes factor.


\subsubsection*{Bayesian Model Averaging}

Instead of choosing between models, it is also possible to perform multi-model predictions by computing a weighted average of individual model predictions weighted by the posterior model probabilities. 
\emph{Bayesian model averaging} (BMA) accounts for model uncertainty. Interesting similarities can be found with bagging, method discussed in \hyperref[sec:bagg]{Chapter \textbf{\ref{sec:bagg}}} which makes an average of bootstrapped models to improve the prediction accuracy. Both relies on the same data but BMA will use different models, and hence correlations between predictions of the different models will be important. It is particularly useful when distinct models provide an acceptable description of the data, but yield different predictions.



\section{Bayesian Predictive Accuracy for Model Checking}\label{sec:predacur}


%\subsection{Cross-validation for predictive accuracy}

If we have a large amount of data, we can use validation techniques by dividing the dataset between a training set (e.g. containing $75\%$ of the whole set), and a test set containing the remaining observations.
For example, having $N$ draws $\theta_1,\theta_2,\dots,\theta_N$ from the posterior $\pi(\theta|\boldsymbol{x}_{\text{train}})$, we can score each value using 

\begin{equation*}
\log\bigg[N^{-1}\sum_{t=1}^N f(x^*|\theta_t)\bigg],
\end{equation*}
and that can be summed for any value $x^*$ in $\boldsymbol{x}_{\text{test}}$. This can be used to compare models, with larger values indicating better models.
However, we often do not have large amounts of data in an EVT context. Henceforth, we can rather use \emph{cross-validation} techniques, but it is a computationally demanding method. There exists several variants, and
\citet{Vehtari_practical_2016} provide an excellent summary.

Or, one could rather prefer information criteria that avoid these computations. This approach is to use something on the form $\log f(\boldsymbol{x}|\bar{\theta})-p^*$ for $N$ draws $\theta_1,\dots,\theta_N$ from $\pi(\theta|\boldsymbol{x})$, where $p^*$ represents the effective number of parameters and $\bar{\theta}$ is the posterior mean. Since $\log f(\boldsymbol{x}|\bar{\theta})$ is the density of the observations given the posterior mean of the parameters, it indicates a good fit.  We will see two existing methods that use this idea.

\subsubsection*{Deviance Information Criterion}

The \emph{Deviance Information Criterion}
(DIC) was first proposed by \citet{Spiegelhalter_bayesian_2002} and use the following to compute the effective number of parameters :

\begin{equation*}
p^*=2\cdot \bigg(\log f(\boldsymbol{x}|\bar{\theta})-N^{-1}\sum_{t=1}^N\log f(\boldsymbol{x}|\theta_t)\bigg).
\end{equation*}
Since the DIC is defined on the deviance scale, we have
\begin{equation}
\text{DIC}= 2\log f(\boldsymbol{x}|\bar{\theta}) - \frac{4}{N} \sum_{t=1}^N\log f(\boldsymbol{x}|\theta_t).
\end{equation}
Hence, smaller DIC values indicate better models. It can be computed simply by storing $f(\boldsymbol{x}|\theta_t) $ at the end of each iteration in the main loop of the MCMC algorithm.


\subsubsection*{Widely Applicable Information Criterion}

The \emph{Widely Applicable Information Criterion} (WAIC) is a more recent approach proposed by \citet{Watanabe_asymptotic_2010}, giving the criterion  


\begin{equation}
\begin{aligned}
\text{WAIC}:= & \ 2 \sum_{i=1}^n\Big[\log\big\{\mathbb{E}_{\theta|x}f(x_i|\theta)\big\}\Big]- \mathbb{E}_{\theta|x}\log f(x_i|\theta) \\
= &\sum_{i=1}^n\Bigg[2\log \bigg(N^{-1}\sum_{t=1}^N f(x_i|\theta_t)\bigg)-\frac{4}{N}\sum_{t=1}^N\log f(x_i|\theta_t)\Bigg].
\end{aligned}
\end{equation}
It can be calculated by computing $\log f(x_i|\theta_t)$ $\forall i=1,\ldots,N$ at the end of each iteration in the main loop of the MCMC algorithm.

There exists quite other interesting methods, as proposed by \citet{gelman_understanding_2014} who give in-depth developments of the predictive information criteria.

 

%\section{Bayesian Model Averaging}


%\section{Bayesian Neural Networks}

